# the deadly recklessness of the self driving car industr 1831027948

> 原文：<https://gizmodo.com/the-deadly-recklessness-of-the-self-driving-car-industr-1831027948>

自动驾驶汽车本应使驾驶更加安全，但它们仍有可能——一些更乐观的研究表明 自动驾驶汽车仅在美国一年就可以挽救数万人的生命。但到目前为止，鲁莽已经定义了追求这项技术的最大公司的文化——优步、谷歌，甚至可以说是特斯拉——并直接导致了不必要的撞车、受伤甚至死亡。

让我们明确这一点，因为在我看来，这些公司在承担一项困难的、潜在的“革命性”技术方面获得了一点通行证，因为在车祸中，责任可能看起来模糊不清，或者在这些情况下，责任可能指向本应看着道路的人。这些公司的行为(或者有时是不作为)导致了生命和肢体的损失。在这个过程中，他们让该领域的整体前景变得黯淡，削弱了公众对自动驾驶汽车的信任，并推迟了许多人希望能够拯救生命的技术的推出。



追求自动驾驶技术的最知名公司的一系列失败突显出一个事实，即自动驾驶系统的安全性取决于建造它们的人和组织。

*   **根据信息 获得的一封邮件**最近 [显示，优步的自动驾驶汽车部门可能不仅仅是鲁莽，而是彻头彻尾的疏忽。据报道，该公司的管理人员无视其安全团队的详细呼吁，继续不安全的做法，一名行人死亡。在此之前，许多事故和未遂事故都没有被注意到。](https://www.theinformation.com/articles/the-uber-whistleblowers-email) 
*   **据报道，谷歌自动驾驶汽车部门至少有一名主要高管**免除了自己的测试程序协议，直接造成了严重的撞车事故，伤害了他的乘客，并且从未通知警方这是由自动驾驶汽车造成的。根据加州车管所的记录，据我统计，现在是谷歌子公司的 Waymo 今年卷入了 21 起报告的撞车事故，尽管它在一起事故中有过错。

*   **在** [**两个单独的场合**](https://www.wired.com/story/tesla-autopilot-self-driving-crash-california/) ，Autopilot，特斯拉的半自动驾驶系统，在司机遭遇致命车祸时启用。10 月，佛罗里达州一名特斯拉车主 [起诉该公司](https://www.wired.com/story/tesla-autopilot-crash-lawsuit-florida-shawn-hudson/) ，称该公司“欺骗了消费者”...相信它为特斯拉汽车提供的自动驾驶系统可以在高速公路上安全地运送乘客，只需乘客最少的输入和监督。”(特斯拉当然反驳这种定性。)这些案例更加模糊，因为特斯拉明确警告不要让系统完全驾驶汽车，并安装了安全措施来阻止这种类型的不良驾驶员行为。然而，特斯拉继续在其网站上宣传其 [提供](https://www.tesla.com/autopilot) “所有汽车上的全自动驾驶硬件”，其自己的工程师告诉监管机构，他们预计一些司机将完全依赖该系统。然而在公开场合，特斯拉继续否认他们的系统可能会让司机对其半自动系统产生任何危险的依赖。

难怪大众大众对自动驾驶汽车心存警惕。

* * *

“目前，美国的测试相当鲁莽，”杰克·斯蒂尔戈博士说，他是伦敦大学学院科学技术部的高级讲师，也是即将到来的无人驾驶未来项目 的首席研究员。"由公司来决定什么样的风险是可接受的。"通常，这些公司已经明确决定非常高的风险是可以接受的。

关于自动驾驶汽车领域的公司可以有多鲁莽的最新和最明显的例子涉及到亚利桑那州坦佩市臭名昭著的致命车祸，其中一辆优步的汽车 [撞死了一名 49 岁的行人](https://gizmodo.com/uber-self-driving-car-killed-arizona-woman-while-in-au-1823891032) 。 [该信息获得了一封电子邮件](https://www.theinformation.com/articles/how-an-uber-whistleblower-tried-to-stop-self-driving-car-disaster) 据报道，这封电子邮件是由测试运营部门的前经理罗比·米勒发送给七名优步高管的，其中包括该公司自动驾驶汽车部门的负责人，警告说为出租车提供动力的软件有问题，后备司机没有得到足够的培训。



米勒写道:“这些汽车经常发生事故，造成损坏。”。“这通常是由于操作员或反病毒技术的不良行为造成的。二月份几乎每隔一天就有一辆车被损坏。我们不应该每行驶 15000 英里就撞上东西。因驾驶技术差而屡次违章很少会导致解雇。几名司机似乎没有经过适当的审查或培训。”

这太疯狂了。当时，在旧金山、匹兹堡、圣达菲和其他地方，有数百辆自动驾驶汽车在路上行驶。反病毒技术显然存在缺陷，备用司机没有保持警惕，尽管一再发生事故——有些显然很危险——但什么也没有解决。在米勒的电子邮件日期的五天后，一辆使用优步自动驾驶软件的沃尔沃撞上了伊莱恩·赫尔茨贝格，当时她正骑着自行车慢慢地过马路，并杀死了她。事故发生时，司机是 [显然是在葫芦](https://gizmodo.com/uber-driver-in-fatal-tempe-crash-may-have-been-watching-1827039127) 上播放*的声音*。

这场悲剧不是某项尖端技术的反常故障，而是完全可以预见的企业渎职的副产品。

如果优步是这种情况下最糟糕的角色，它也不是唯一一个糟糕的角色——它建立在多年前建立的文化基础上，在那里，对技术领先的需求盖过了安全问题。



安东尼·莱万多夫斯基(Anthony Levandowski)是谷歌自动驾驶汽车项目的前负责人，他的鲁莽、粗心和鲁莽是出了名的。或许可以说，我们甚至不知道他监管的自动驾驶汽车发生了多少起碰撞。这里有一个关键的例子，正如查尔斯·杜希格在《纽约客》 中报道的 [:](https://www.newyorker.com/magazine/2018/10/22/did-uber-steal-googles-intellectual-property)

> “2011 年的一天，一位名叫艾萨克·泰勒的谷歌高管得知，在他休陪产假期间，莱万多夫斯基修改了汽车的软件，这样他就可以让汽车行驶在原本禁止行驶的路线上。一位谷歌高管回忆说，他看到泰勒和莱万多夫斯基对着对方大喊大叫。莱万多夫斯基告诉泰勒，向他展示为什么他的方法是必要的唯一方式是一起乘车。这两个人仍然很愤怒，他们跳上一辆自动驾驶的普锐斯车，扬长而去。
> 
> “汽车上了高速公路，经过了一个入口匝道。据了解当天事件的人说，普锐斯意外撞上了另一辆车，一辆凯美瑞。人类司机可以通过减速并让凯美瑞融入交通来轻松处理这种情况，但谷歌的软件没有为这种情况做好准备。汽车继续并排在高速公路上飞驰。凯美瑞的司机猛地把车开到右肩上。然后，显然是为了避开护栏，他转向左边；那辆凯美瑞滑行着穿过高速公路，进入了中央隔离带。担任安全驾驶员的莱万多夫斯基拼命转向以避免与凯美瑞相撞，导致泰勒脊椎严重受伤，最终需要多次手术。
> 
> “普锐斯恢复了控制，在高速公路上拐了个弯，把凯美瑞甩在了后面。莱万多夫斯基和泰勒不知道凯美瑞损坏得有多严重。他们没有回去查看另一名司机，或者看看是否还有其他人受伤。他们和谷歌的其他高管都没有向当局询问。警方没有被告知自动驾驶算法导致了这起事故。”



莱万多夫斯基倾向于将反病毒测试置于安全之前——现在是——这一点得到了很好的证明，而警方并未被告知这一事实是这里的关键部分。谷歌在很长一段时间内似乎对其自动驾驶汽车项目保持坦诚；以至于 [Wired 报道称，在莱万多夫斯基事件五年后的 2016 年](https://www.wired.com/2016/02/googles-self-driving-car-may-caused-first-crash/) ，一辆谷歌汽车导致了其“第一次撞车”。

这一点，再加上优步事件的曝光，应该让我们深入思考，我们应该从在我们的共享道路上进行自动驾驶汽车测试的公司那里要求什么程度的信任和透明度。

然而，在 2016 年 12 月，谷歌将其自动驾驶汽车部门拆分为 Waymo，该公司尚未报告类似的严重撞车事件。“安东尼·莱万多基(Anthony Levandowki)对安全的忽视并没有反映出我们在 Waymo 的使命和价值观，我们团队中的数百名工程师每天都在努力将这项技术安全地带到我们的道路上，”一位发言人在一封电子邮件中告诉我。“我们公司的成立是为了改善道路安全，因此我们坚持高安全标准。”根据加州自动驾驶汽车事故日志，Waymo 今年卷入了 21 起轻微事故，只在一起事故中有过错。这是一个相当好的记录，假设所有的事故都被准确报道，如果 Waymo 希望重新获得公众的信任，它对安全的强调似乎为该行业指明了前进的方向。

就其本身而言，优步已经让自动驾驶汽车上路，并表示已经彻底检查了自动驾驶汽车测试程序。不过，当我问优步的一位发言人，事故发生后，他们是如何改变公司政策的，她给我的回复与给信息台( [和我的同事詹宁斯·布朗](https://gizmodo.com/uber-employee-warned-self-driving-cars-are-routinely-in-1831019048) )的回复一模一样:“现在，整个团队都专注于以自动驾驶模式安全、负责任地重返道路。我们对车队为实现目标所做的工作充满信心。我们的团队仍然致力于实施关键的安全改进，我们打算只有在这些改进已经实施并且我们已经获得宾夕法尼亚州交通部的授权时，才能恢复上路自动驾驶测试。”



优步拒绝回答关于公司文化是否有所改变，公司是否有人被追究责任，或者除了那份声明和 [公开发布的材料](https://medium.com/@UberATG/self-driving-cars-return-to-pittsburgh-roads-in-manual-mode-f83e506a04b9) 之外的任何事情。优步没有否认对此次事故负有责任。

* * *

在 2018 年 12 月的第一周，警方成功拦下了一辆特斯拉 Model S，其司机在以 70 英里/小时的速度行驶时睡着了。他让 [自动驾驶启用了](https://observer.com/2018/12/tesla-drunk-driving-autopilot-alexander-samek/) ，在被拦下之前已经行驶了 17 英里。在前面提到的 10 月份对特斯拉的诉讼中，佛罗里达州男子肖恩·哈德森(Shawn Hudson)声称，他被误导 认为汽车可以自动驾驶，而他的特斯拉在自动驾驶状态下坠毁。他说，他买特斯拉的部分原因是因为它的自动驾驶功能，他认为这可以让他在漫长的通勤途中放松一下，所以在路上会定期写电子邮件和查看手机——包括车祸发生的时候。

特斯拉处于一个独特的位置。如前所述，该公司长期以来一直在运输其被称为“全自动驾驶硬件”的汽车，同时还发布咨询意见，不要完全依赖 Autopilot，这是一种额外花费 5000 美元的半自动驾驶系统。它不断升级其软件，让已经上路的司机更接近无人驾驶的现实。

“特斯拉，”斯蒂尔戈告诉我，“对他们的司机自己的自动驾驶实验视而不见。人们正在不负责任地使用 Autopilot，而特斯拉却在忽视它，因为他们正在收集数据。而特斯拉也在误导人们，说他们在卖自动驾驶汽车。他们用‘全自动驾驶硬件’这个词。”



事实上，埃隆·马斯克(Elon Musk)本人本质上是在宣传特斯拉的无人驾驶潜力，他大胆地在不少于[*60 分钟*亮相](https://www.wired.com/story/elon-musk-tesla-autopilot-60-minutes-interview/) 的地方做了一点广告。特斯拉强调，在购买过程中，其销售团队展示了自动驾驶仪的正确使用，消除了买家对它的功能类似于自动驾驶汽车的看法。它增加了一些功能，如果用户长时间不用手，就会关闭自动驾驶。尽管如此，营销和建议的力量——更不用说长期以来极客乘坐自动驾驶汽车的梦想——是强大的力量，自动驾驶被滥用为自动驾驶系统的事故和事件仍在继续。

以这种方式公开营销其产品是一个有意识的决定，它可能有助于向特斯拉司机灌输信心，他们可以按照营销语言描述的方式使用该功能。 [问题是](https://jalopnik.com/this-test-shows-why-tesla-autopilot-crashes-keep-happen-1826810902) “当司机过度依赖系统时会发生什么，”Thatcham Research 解释道，该公司调查了该软件在[BBC](https://www.bbc.com/news/technology-44439523)的一个片段中到底有多好。(剧透警告:有许多场景中，Teslas 在自动驾驶仪上运行时撞到了他们前面的物体。)但这并不能阻止它们体内的人类陷入一种虚假的安全感。

这是关于特斯拉对其司机的责任的第二个问题——它自己的工程师 [知道这是一个明显的安全问题](https://jalopnik.com/tesla-knew-drivers-would-be-inattentive-idiots-with-aut-1791431556) 。(这也是为什么我认为把所有的责任都堆在“安全司机”身上有点不公平，在优步致命的车祸中，他本应该看着路；一方面，优步显然将安全司机从两个减少到一个——米勒呼吁恢复两个司机——另一方面，随着越来越多的证据表明，在几个小时的无事件之后，我们是安全的，在我们变得非常非常无聊之后，人类的本性是被麻痹到一种安全感中。)事实上，Waymo 最近的一次撞车事故发生在它唯一的安全司机 [在驾驶](https://qz.com/1410928/waymos-self-driving-car-crashed-because-its-human-driver-fell-asleep/) 时睡着了，并意外关闭了系统。



在第一次致命车祸后发布的报告中，美国国家公路交通安全管理局(National Highway Traffic Safety Administration)澄清了特斯拉的不法行为，但 [发现](https://static.nhtsa.gov/odi/inv/2016/INCLA-PE16007-7876.PDF) :

> “在研究和开发 Autopilot 的过程中，特斯拉考虑了驾驶员可能以各种方式滥用系统的可能性，包括上面确定的方式，即通过模式混淆、分心驾驶以及在首选环境和条件之外使用系统。特斯拉工程师认为，司机分心的类型包括司机可能在使用 Autopilot 时注意力不集中、睡着或变得无法工作。作为特斯拉设计流程的一部分，我们评估了驱动程序误用的可能性，并对解决方案进行了测试、验证和整合，以广泛发布产品。看起来，特斯拉对驾驶员滥用的评估及其导致的行动解决了这种滥用可能带来的不合理的安全风险。”

换句话说，特斯拉的团队知道一些司机将使用 Autopilot 作为实际的自动驾驶模式，否则最终会完全依赖该系统。虽然 NHTSA 发现特斯拉的工程师已经考虑到了这些因素，但该系统仍然在两次致命车祸和更多事故中使用。(与此同时，美国国家运输安全委员会发现 [事实上部分责任在于特斯拉](https://www.wired.com/story/tesla-ntsb-autopilot-crash-death/) ，因为特斯拉没有尽力劝阻司机滥用自动驾驶功能)。

在与特斯拉发言人进行了数小时的电话通话后，该公司拒绝公开谈论“所有汽车上都有完全自动驾驶硬件”这样的语言来宣传其车辆，然后坚持购买特斯拉的司机不要将它们视为自动驾驶汽车(其中一位发言人友好地告诉我，听起来我需要再思考一下我的故事)，该公司只提供了它在 10 月份佛罗里达州诉讼后发布的声明。声明如下:“特斯拉一直很清楚，自动驾驶并不能让汽车不受所有事故的影响，特斯拉竭尽全力提供关于自动驾驶是什么和不是什么的明确说明，包括在车主试驾和提车时，在司机启用自动驾驶之前，每次使用自动驾驶时，以及通过车主手册和软件更新的发行说明向司机提供说明。”



听着，开车已经是一件又脏又危险的事情了。从人类健康的角度来看，高性能的自动驾驶汽车可能是天赐之物，在交通中测试它们将永远是后勤和伦理上的一个棘手问题。在特斯拉的案例中，我们无法知道一些自动驾驶用户是否会如此分心地驾驶，以至于他们会不顾一切地造成事故。但事实是，对于我们如何与自动驾驶汽车共存，我们这些已经在路上开着不那么自动驾驶的汽车的人几乎没有发言权。关于我们是否正在与运行劣质软件的 AVs 共享街道，或者由不太警觉的人类司机监督，因为高管们不想在寻求真空处理道路数据或错过销售机会方面落后。

到目前为止，正如莱万多夫斯基 所说，处于“他们需要赢得的竞赛”中的科技巨头们已经为我们做出了那些决定。除了推动立法以更严格地监管自动驾驶汽车的测试，要求制造伤害和致命违规行为的公司承担适当的责任似乎是一种合理的手段。除非这些公司大幅改善努力，将安全和开放置于开发速度和推销之上，否则它们可能会造成进一步的伤害，并疏远已经对自动驾驶汽车前景心存警惕的公众。

“我们仍然不知道大多数人会认为什么是可接受的风险，但我的猜测是，即使我们知道自动驾驶汽车比人类司机安全一点，公众也不会满意，”Stilgoe 说。“坠机，任何坠机，都是一场灾难。自动驾驶汽车行业必须更加努力，才能赢得关于安全性的争论。”

* * *

更正:这篇文章的前一个版本称，在 BBC 一篇测试车辆的特别报道中，“全自动驾驶”特斯拉撞上了他们前面的物体。特斯拉声称，尽管其车辆拥有“完全自动驾驶能力所需的”硬件，但这些汽车目前还不能完全自动驾驶。我们对这个错误感到抱歉。